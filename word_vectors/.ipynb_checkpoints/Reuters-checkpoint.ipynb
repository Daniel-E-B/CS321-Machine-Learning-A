{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Conv1D, Dense, Flatten, Dropout, MaxPooling1D\n",
    "from keras.datasets import reuters\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from gensim.models import word2vec\n",
    "import numpy\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from IPython.display import SVG\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", \n",
    "                                                         num_words=None, \n",
    "                                                         skip_top=0, \n",
    "                                                         maxlen=None, \n",
    "                                                         test_split=0.2, \n",
    "                                                         seed=113, \n",
    "                                                         start_char=1, \n",
    "                                                         oov_char=2, \n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 3\n",
    "reuters_map = dict((index + offset, word) for (word, index) in reuters.get_word_index().items())\n",
    "reuters_map[0] = 'PADDING'\n",
    "reuters_map[1] = 'START'\n",
    "reuters_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'START mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([reuters_map[word_index] for word_index in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [['PADDING'] + [reuters_map[word_index] for word_index in review] for review in x_train]\n",
    "test_sentences = [['PADDING'] + [reuters_map[word_index] for word_index in review] for review in x_test]\n",
    "# test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_wv_model = word2vec.Word2Vec(train_sentences + test_sentences + ['UNKNOWN'], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29345626, -0.7750187 ,  0.22822663, ..., -0.08673308,\n",
       "        -3.2106571 , -0.08949522],\n",
       "       [-1.9518943 ,  0.48839745, -1.7265432 , ..., -0.65465045,\n",
       "        -2.2256184 , -0.5999684 ],\n",
       "       [ 0.40988147, -0.7328701 , -0.45011625, ...,  1.1804185 ,\n",
       "        -2.887608  , -0.29957724],\n",
       "       ...,\n",
       "       [-0.03466025,  0.05016937,  0.02303992, ..., -0.06655001,\n",
       "        -0.02845289, -0.05237921],\n",
       "       [-0.03770472,  0.06859544,  0.02165744, ..., -0.08396146,\n",
       "        -0.03245724, -0.07030506],\n",
       "       [-0.03494635,  0.05537092,  0.02854297, ..., -0.07413835,\n",
       "        -0.03133663, -0.04956329]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_wordvec = reuters_wv_model.wv\n",
    "reuters_wv_model.wv.get_vector('snake')\n",
    "\n",
    "reuters_wv_model.wv.vectors # list of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest review: 2376 Shortest review: 2\n"
     ]
    }
   ],
   "source": [
    "# shorten and pad\n",
    "lengths = [len(review) for review in x_train.tolist() + x_test.tolist()]\n",
    "print('Longest review: {} Shortest review: {}'.format(max(lengths), min(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 reviews out of 11228 are over 500.\n"
     ]
    }
   ],
   "source": [
    "cutoff = 500\n",
    "print('{} reviews out of {} are over {}.'.format(\n",
    "    sum([1 for length in lengths if length > cutoff]), \n",
    "    len(lengths), \n",
    "    cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train_padded = sequence.pad_sequences(x_train, maxlen=cutoff)\n",
    "x_test_padded = sequence.pad_sequences(x_test, maxlen=cutoff)\n",
    "len(x_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = reuters_wordvec.get_keras_embedding(train_embeddings=False)\n",
    "embedding_layer.input_length = cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that embedding layer works the same as regular wordvec\n",
    "model.predict(numpy.array([[reuters_wordvec.vocab[\"W\"].index]]))[0][0] == reuters_wordvec[\"W\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3, ..., 25,  3, 25])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train # 0 to 45 (46 categories)... need to onehot encode so that we can do softmax with a good fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_enc = LabelBinarizer()\n",
    "y_train_onehot = onehot_enc.fit_transform(y_train)\n",
    "y_test_onehot = onehot_enc.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=50, strides=2, kernel_size=5, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=50, strides=2, kernel_size=7, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=50, strides=2, kernel_size=5, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=50, strides=2, kernel_size=7, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=46, activation='softmax')) # 46 topics\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/1\n",
      "8982/8982 [==============================] - 15s 2ms/step - loss: 2.9921 - categorical_accuracy: 0.2829 - val_loss: 3.2654 - val_categorical_accuracy: 0.3620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.26543, saving model to weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127febc18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpt = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.fit(x_train_padded, y_train_onehot, epochs=1, batch_size=128, validation_data=(x_test_padded, y_test_onehot), callbacks=[checkpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(y_train, bins=46)\n",
    "plt.show() # guess reuters has a lot of articles about the same topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 2s 778us/step\n",
      "loss: 1.492768557923029 accuracy: 0.6393588601959038\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"BEST_reuters_weights_64.hdf5\") # trained 1000 epochs on bandersnatch\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "scores = model.evaluate(x_test_padded, y_test_onehot)\n",
    "print('loss: {} accuracy: {}'.format(*scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
