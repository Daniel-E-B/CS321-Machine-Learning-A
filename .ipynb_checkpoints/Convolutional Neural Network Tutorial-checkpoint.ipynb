{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Type `python3 --version` into Terminal. If the output starts with \"Python 3.6\", skip to step 3. **Tensorflow does not yet work with Python 3.7, so you _must_ get Python 3.6.** See https://github.com/tensorflow/tensorflow/issues/20517 for updates on 3.7 support.\n",
    "2. Go to https://www.python.org/downloads/ and click on \"Python 3.6.8\". Scroll down to the Files section and click on \"macOS 64-bit installer\". Run the installer and follow the directions. Repeat step 1 to make sure it has successfully installed. \n",
    "3. Install jupyter notebook, so that you can use this tutorial, by typing the following into Terminal:\n",
    "    `pip3 install jupyter`\n",
    "4. Start the jupyter notebook by typing in Terminal _in the same folder that you have this file_ \n",
    "    `jupyter notebook`\n",
    "    This should open a tab in your web browser with a list of files in the folder. Click on this ipynb file to open it.\n",
    "3. Install the tensorflow machine learning library by typing the following into Terminal:\n",
    "    `pip3 install --upgrade tensorflow`\n",
    "4. Install the keras machine learning library by typing the following into Terminal:\n",
    "    `pip3 install keras`\n",
    "5. Install the libraries we'll need to display the images: `pip3 install numpy matplotlib`\n",
    "6. Test that the keras install worked: Type `python3` into the Terminal. When the `>>>` prompt comes up, type `from keras.models import Sequential`. If you don't get any error output, then it worked. Type Ctrl+d (or close the window) to exit.\n",
    "    * If you get an error like `ModuleNotFoundError: No module named 'theano'` then you need to switch the backend to tensorflow. See the instructions at https://keras.io/backend/ or ask me for help.\n",
    "    * If you get a warning like `/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6` you can ignore it. This is a known (trivial) issue with Tensorflow 1.4 for OSX. See https://github.com/tensorflow/tensorflow/issues/14182 if you'd like more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code\n",
    "\n",
    "First, we'll want to import the keras modules we'll be using for our neural network and the numpy and matplotlib modules that we'll be using for displaying our test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "import numpy\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# tell matplotlib to display images within this notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Model\n",
    "\n",
    "Next, let's set up the structure of our model. We'll start with a really simple model, with just one convolutional layer that has just one filter. We are going to be using 9x9-pixel grayscale images, so we set the input shape accordingly. If we were using color images with red-green-blue channels, the last dimension would be size three (one for each color) instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "image_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(Conv2D(filters=1,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=1,\n",
    "                  input_shape=(image_size, image_size, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally at this point, we would compile and train (aka fit) our model, but instead we're going to set the weights manually and then see the output we get on some test images.\n",
    "\n",
    "First, let's take a look at what the randomly generated weights look like, to understand the format that we'll need to use to set the new weights. By changing the parameters of the model above and looking at how it affects the weight structure, we can understand what each weight is connected to (try it!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.372374  ]],\n",
       " \n",
       "         [[-0.23576069]],\n",
       " \n",
       "         [[-0.4345278 ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03834862]],\n",
       " \n",
       "         [[-0.25187963]],\n",
       " \n",
       "         [[-0.33813548]]],\n",
       " \n",
       " \n",
       "        [[[-0.22303638]],\n",
       " \n",
       "         [[ 0.12822026]],\n",
       " \n",
       "         [[ 0.29573345]]]], dtype=float32), array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model0.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we change the weights so that the filter will capture a certain pattern. We'll explore more about what this means below, but feel free to start generating some guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 1.]],\n",
       " \n",
       "         [[ 1.]],\n",
       " \n",
       "         [[ 1.]]],\n",
       " \n",
       " \n",
       "        [[[-1.]],\n",
       " \n",
       "         [[-1.]],\n",
       " \n",
       "         [[-1.]]],\n",
       " \n",
       " \n",
       "        [[[-1.]],\n",
       " \n",
       "         [[-1.]],\n",
       " \n",
       "         [[-1.]]]], dtype=float32), array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_num = 0\n",
    "filter_num = 0\n",
    "y = 0\n",
    "for x in range(kernel_size):\n",
    "    weights[layer_num][y][x][0][filter_num] = 1\n",
    "for y in range(1,kernel_size):\n",
    "    for x in range(kernel_size):\n",
    "        weights[layer_num][y][x][0][filter_num] = -1\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save those weights back into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Images\n",
    "\n",
    "Now, let's create some 9x9 images that we will run through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13148c240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image0 = numpy.array([\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "    [128, 0, 128, 255, 128, 0, 128, 255, 128],\n",
    "], dtype=numpy.uint8)\n",
    "imshow(image0, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1315a7048>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image1 = numpy.array([\n",
    "    [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "], dtype=numpy.uint8)\n",
    "imshow(image1, cmap='gray')\n",
    "\n",
    "# image1 = numpy.array([\n",
    "#     [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "#     [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "#     [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "#     [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#     [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "#     [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "#     [128, 128, 128, 128, 128, 128, 128, 128, 128],\n",
    "# ], dtype=numpy.uint8)\n",
    "# imshow(image1, cmap='gray')\n",
    "\n",
    "# I messed with the commented out one to confirm my theories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Images Through Model\n",
    "\n",
    "The images need to be in a slightly different format for Keras than they do for the imshow command. Right now, they are 9x9 arrays, and we need them to be 9x9x1 -- three dimensional instead of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for image in [image0, image1]: # You may find it easier to take one of these out, to look at them one at a time\n",
    "    images.append(numpy.resize(image, (image_size, image_size, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we give these images to our model and take a look at what the filter has found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]],\n",
       "\n",
       "        [[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]],\n",
       "\n",
       "        [[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]],\n",
       "\n",
       "        [[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]],\n",
       "\n",
       "        [[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]],\n",
       "\n",
       "        [[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]],\n",
       "\n",
       "        [[ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.],\n",
       "         [ -383.],\n",
       "         [ -256.],\n",
       "         [ -383.],\n",
       "         [ -511.]]],\n",
       "\n",
       "\n",
       "       [[[    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.]],\n",
       "\n",
       "        [[-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.]],\n",
       "\n",
       "        [[ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.]],\n",
       "\n",
       "        [[  381.],\n",
       "         [  381.],\n",
       "         [  381.],\n",
       "         [  381.],\n",
       "         [  381.],\n",
       "         [  381.],\n",
       "         [  381.]],\n",
       "\n",
       "        [[    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.],\n",
       "         [    0.]],\n",
       "\n",
       "        [[-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.],\n",
       "         [-1149.]],\n",
       "\n",
       "        [[ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.],\n",
       "         [ -765.]]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.predict(numpy.array(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions -- Answer these before going on to the second model!\n",
    "1. There are a lot of numbers in the output above: 2 arrays of 7 arrays of 7 arrays of a single element. Why are they in groups of seven?\n",
    "\n",
    "They are in groups of 7 because the kernels are 3x3, and cannot run unless it is on a grid of 3x3 pixels (of which there are only 7 per row with this stride size).\n",
    "\n",
    "2. When we created the model, we asked it to have one filter. In which image is the filter \"finding\" something? How do you know? How does this relate to the pattern of weights that was set?\n",
    "\n",
    "The pattern of weights sets the weights for the first row of the kernel to 1 and the second two to -1. The kernel's output will be 0 when the top input row is the sum of the bottom two input rows. Becuase gray is 128 and black is 0, gray black gray satisfies this condition. White black white also satisfies this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Second Model\n",
    "\n",
    "Let's move to a slightly more complex model. Now, there are two convolutional layers, the first with two filters and the second with one filter. One other difference is that we're going to be taking strides so that we only examine each pixel once, instead of looking at overlapping groups. This makes it a little simpler to understand the manual weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(filters=2,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=(3,3),\n",
    "                  input_shape=(image_size, image_size, 1)))\n",
    "model1.add(Conv2D(filters=1, kernel_size=kernel_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a different model structure, we will have a different number of weights to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.16374502,  0.02671954]],\n",
       " \n",
       "         [[-0.09816697,  0.46719334]],\n",
       " \n",
       "         [[-0.06104955, -0.29296792]]],\n",
       " \n",
       " \n",
       "        [[[-0.42369217, -0.1156396 ]],\n",
       " \n",
       "         [[-0.22666565, -0.15840775]],\n",
       " \n",
       "         [[ 0.03742459,  0.06751081]]],\n",
       " \n",
       " \n",
       "        [[[ 0.36225137, -0.44776934]],\n",
       " \n",
       "         [[ 0.25517502,  0.03743586]],\n",
       " \n",
       "         [[ 0.18731186,  0.01441884]]]], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([[[[-0.30245513],\n",
       "          [-0.39923382]],\n",
       " \n",
       "         [[ 0.13332596],\n",
       "          [ 0.20984688]],\n",
       " \n",
       "         [[ 0.46762535],\n",
       "          [-0.11889178]]],\n",
       " \n",
       " \n",
       "        [[[ 0.04360929],\n",
       "          [ 0.46540597]],\n",
       " \n",
       "         [[-0.06310406],\n",
       "          [ 0.46898642]],\n",
       " \n",
       "         [[ 0.17303583],\n",
       "          [-0.14475745]]],\n",
       " \n",
       " \n",
       "        [[[ 0.24664256],\n",
       "          [ 0.01786476]],\n",
       " \n",
       "         [[ 0.4654127 ],\n",
       "          [-0.12790614]],\n",
       " \n",
       "         [[ 0.40591404],\n",
       "          [-0.24957487]]]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we manually set the weights to match some specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 1., -1.]],\n",
       " \n",
       "         [[-1., -1.]],\n",
       " \n",
       "         [[-1.,  1.]]],\n",
       " \n",
       " \n",
       "        [[[-1., -1.]],\n",
       " \n",
       "         [[ 1.,  1.]],\n",
       " \n",
       "         [[-1., -1.]]],\n",
       " \n",
       " \n",
       "        [[[-1.,  1.]],\n",
       " \n",
       "         [[-1., -1.]],\n",
       " \n",
       "         [[ 1., -1.]]]], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([[[[-0.30245513],\n",
       "          [-0.39923382]],\n",
       " \n",
       "         [[ 0.13332596],\n",
       "          [ 0.20984688]],\n",
       " \n",
       "         [[ 0.46762535],\n",
       "          [-0.11889178]]],\n",
       " \n",
       " \n",
       "        [[[ 0.04360929],\n",
       "          [ 0.46540597]],\n",
       " \n",
       "         [[-0.06310406],\n",
       "          [ 0.46898642]],\n",
       " \n",
       "         [[ 0.17303583],\n",
       "          [-0.14475745]]],\n",
       " \n",
       " \n",
       "        [[[ 0.24664256],\n",
       "          [ 0.01786476]],\n",
       " \n",
       "         [[ 0.4654127 ],\n",
       "          [-0.12790614]],\n",
       " \n",
       "         [[ 0.40591404],\n",
       "          [-0.24957487]]]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_num = 0\n",
    "filter_num = 0\n",
    "for y in range(kernel_size):\n",
    "    for x in range(kernel_size):\n",
    "        if y == x: # diagonals. 1 if y = x. -1 if not.\n",
    "            weights[layer_num][y][x][0][filter_num] = 1\n",
    "        else:\n",
    "            weights[layer_num][y][x][0][filter_num] = -1\n",
    "\n",
    "filter_num = 1\n",
    "for y in range(kernel_size):\n",
    "    for x in range(kernel_size):\n",
    "        if kernel_size - 1 - y == x: # diagonal lines going the other way (top right to bottom left instead of top left to bottom right)\n",
    "            weights[layer_num][y][x][0][filter_num] = 1\n",
    "        else:\n",
    "            weights[layer_num][y][x][0][filter_num] = -1\n",
    "            \n",
    "# this modifies the weights of the first conv2d layer\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 1., -1.]],\n",
       " \n",
       "         [[-1., -1.]],\n",
       " \n",
       "         [[-1.,  1.]]],\n",
       " \n",
       " \n",
       "        [[[-1., -1.]],\n",
       " \n",
       "         [[ 1.,  1.]],\n",
       " \n",
       "         [[-1., -1.]]],\n",
       " \n",
       " \n",
       "        [[[-1.,  1.]],\n",
       " \n",
       "         [[-1., -1.]],\n",
       " \n",
       "         [[ 1., -1.]]]], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([[[[ 1.  ],\n",
       "          [-0.25]],\n",
       " \n",
       "         [[-0.25],\n",
       "          [-0.25]],\n",
       " \n",
       "         [[-0.25],\n",
       "          [ 1.  ]]],\n",
       " \n",
       " \n",
       "        [[[-0.25],\n",
       "          [-0.25]],\n",
       " \n",
       "         [[ 1.  ],\n",
       "          [ 1.  ]],\n",
       " \n",
       "         [[-0.25],\n",
       "          [-0.25]]],\n",
       " \n",
       " \n",
       "        [[[-0.25],\n",
       "          [ 1.  ]],\n",
       " \n",
       "         [[-0.25],\n",
       "          [-0.25]],\n",
       " \n",
       "         [[ 1.  ],\n",
       "          [-0.25]]]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_num = 2 # why does this skip to 2?\n",
    "filter_num = 0\n",
    "for y in range(kernel_size):\n",
    "    for x in range(kernel_size):\n",
    "        input_filter_num = 0 # first input (probably the first output from last layer (filter0))\n",
    "        if y == x: # diagonal top left to bottom right\n",
    "            weights[layer_num][y][x][input_filter_num][filter_num] = 1\n",
    "        else:\n",
    "            weights[layer_num][y][x][input_filter_num][filter_num] = -0.25\n",
    "        input_filter_num = 1\n",
    "        if kernel_size - 1 - y == x: # diagonal top right to bottom left\n",
    "            weights[layer_num][y][x][input_filter_num][filter_num] = 1\n",
    "        else:\n",
    "            weights[layer_num][y][x][input_filter_num][filter_num] = -0.25\n",
    "# this \n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the weights back into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, run our test images through the model to see what the filters output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(images):\n",
    "    resized_images = []\n",
    "    for image in images:\n",
    "        resized_images.append(numpy.resize(image, (image_size, image_size, 1)))\n",
    "    return model1.predict(numpy.array(resized_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1150.]]],\n",
       "\n",
       "\n",
       "       [[[-1150.]]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_images([image0, image1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Note above that neither image0 nor image1 gets a positive output. Create some images that do get positive ouputs from this model. The code below might help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1317142b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_black = numpy.array([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "], dtype=numpy.uint8)\n",
    "imshow(image_black, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131765e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_white = numpy.array([\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "    [255, 255, 255, 255, 255, 255, 255, 255, 255],\n",
    "], dtype=numpy.uint8)\n",
    "imshow(image_white, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[    0.]]],\n",
       "\n",
       "\n",
       "       [[[-2295.]]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_images([image_black, image_white])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[5865.]]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_test0 = numpy.array([\n",
    "    [255, 0, 0, 0, 255, 0, 0, 0, 255],\n",
    "    [0, 255, 0, 255, 0, 255, 0, 255, 0],\n",
    "    [0, 0, 255, 0, 255, 0, 255, 0, 0],\n",
    "    [0, 255, 0, 0, 0, 0, 0, 255, 0],\n",
    "    [255, 0, 255, 0, 255, 0, 255, 0, 255],\n",
    "    [0, 255, 0, 0, 0, 0, 0, 255, 0],\n",
    "    [0, 0, 255, 0, 255, 0, 255, 0, 0],\n",
    "    [0, 255, 0, 255, 0, 255, 0, 255, 0],\n",
    "    [255, 0, 0, 0, 255, 0, 0, 0, 255],\n",
    "], dtype=numpy.uint8)\n",
    "imshow(image_test0, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "\n",
    "predict_images([image_test0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of above results and how model1 works:\n",
    "\n",
    "For the sake of clarity™️, layer 0 filter 0 will be referred to as L0F0 (this is the top left to bottom right of 1's, with everything else -1), layer 0 filter 1 will be referred to as L0F1 (top right to bottom left of 1's, all else -1), layer 1 filter 0 input 0 will be referred to as L1F0I0 (takes output of L0F0 as input, top right to bottom left of 1's, all else -.25), and layer 1 filter 0 input 1 will be referred to as L1F0I1 (takes output of L0F1 as input, top right to bottom left of 1's, all else -.25).\n",
    "\n",
    "This has pretty close to if not the maximum activation for these filters.\n",
    "\n",
    "After messing around with a 3x3 example we came up with, and manually understanding how the filters and layers work, we (Daniel A and I) worked this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (Optional)\n",
    "\n",
    "Add additional filters to the model, and create images that get positive weights for different patterns of filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist without looking at an mnist tutorial\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial x_train shape: (60000, 28, 28)\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "60000 train samples\n",
      "initial y_train shape: (60000,)\n",
      "5\n",
      "y_train shape: (60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "print('initial x_train shape:', x_train.shape)\n",
    "\n",
    "\n",
    "img_size = 28\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_size, img_size, 1) # put vals in their own array\n",
    "x_test = x_test.reshape(x_test.shape[0], img_size, img_size, 1)\n",
    "input_shape = (img_size, img_size, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "\n",
    "# convert class vectors to binary class matrices (this is keras's good version of onehot)\n",
    "\n",
    "print('initial y_train shape:', y_train.shape)\n",
    "print(y_train[0])\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(y_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (60000, 10)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "  928/60000 [..............................] - ETA: 2:23 - loss: 1.5375 - acc: 0.4871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-17562867788a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "model.save('my_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
